{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOY_DATA_DIR = BASE_DIR + \"data/wmt14_de_en/toy_data/\"\n",
    "TRAIN_EN_TOY = TOY_DATA_DIR + \"train.toy.en\"\n",
    "TRAIN_DE_TOY = TOY_DATA_DIR + \"train.toy.de\"\n",
    "VAL_EN_TOY = TOY_DATA_DIR + \"val.toy.en\"\n",
    "VAL_DE_TOY = TOY_DATA_DIR + \"val.toy.de\"\n",
    "TEST_EN_TOY = TOY_DATA_DIR + \"test.toy.en\"\n",
    "TEST_DE_TOY = TOY_DATA_DIR + \"test.toy.de\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_dataset(src_file, tgt_file, train_ratio=0.8, val_ratio=0.1, sample_fraction=0.01):\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(TOY_DATA_DIR, exist_ok=True)\n",
    "\n",
    "    # Load a fraction of data\n",
    "    with open(src_file, \"r\", encoding=\"utf-8\") as src_f, open(tgt_file, \"r\", encoding=\"utf-8\") as tgt_f:\n",
    "        src_lines = src_f.readlines()\n",
    "        tgt_lines = tgt_f.readlines()\n",
    "\n",
    "    # Ensure both files have the same length\n",
    "    assert len(src_lines) == len(tgt_lines), \"Source and target files must have the same number of lines.\"\n",
    "\n",
    "    # Sample a subset of lines\n",
    "    sample_size = int(len(src_lines) * sample_fraction)\n",
    "    sampled_indices = random.sample(range(len(src_lines)), sample_size)\n",
    "    sampled_src = [src_lines[i] for i in sampled_indices]\n",
    "    sampled_tgt = [tgt_lines[i] for i in sampled_indices]\n",
    "\n",
    "    # Split into train, val, test\n",
    "    train_size = int(len(sampled_src) * train_ratio)\n",
    "    val_size = int(len(sampled_src) * val_ratio)\n",
    "\n",
    "    train_src, val_src, test_src = sampled_src[:train_size], sampled_src[train_size:train_size + val_size], sampled_src[train_size + val_size:]\n",
    "    train_tgt, val_tgt, test_tgt = sampled_tgt[:train_size], sampled_tgt[train_size:train_size + val_size], sampled_tgt[train_size + val_size:]\n",
    "\n",
    "    # Save toy datasets\n",
    "    with open(TRAIN_EN_TOY, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(train_src)\n",
    "    with open(TRAIN_DE_TOY, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(train_tgt)\n",
    "    with open(VAL_EN_TOY, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(val_src)\n",
    "    with open(VAL_DE_TOY, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(val_tgt)\n",
    "    with open(TEST_EN_TOY, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(test_src)\n",
    "    with open(TEST_DE_TOY, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(test_tgt)\n",
    "\n",
    "    print(f\"Toy dataset created with {sample_size} samples (train: {train_size}, val: {val_size}, test: {len(test_src)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_EN_FILE = BASE_DIR + \"data/wmt14_de_en/train.tok.clean.bpe.32000.en\"\n",
    "TRAIN_DE_FILE = BASE_DIR + \"data/wmt14_de_en/train.tok.clean.bpe.32000.de\"\n",
    "VOCAB_FILE = BASE_DIR + \"data/wmt14_de_en/vocab.bpe.32000\"\n",
    "VOCAB_SIZE = 32000\n",
    "\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_UNITS = 512\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy dataset created with 4500 samples (train: 3600, val: 450, test: 450)\n"
     ]
    }
   ],
   "source": [
    "create_toy_dataset(TRAIN_EN_FILE, TRAIN_DE_FILE, train_ratio=0.8, val_ratio=0.1, sample_fraction=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 450 samples.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the vocabulary and add <sos> and <eos> tokens if not present\n",
    "def load_vocab(vocab_file):\n",
    "    vocab = {}\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for idx, token in enumerate(f):\n",
    "            vocab[token.strip()] = idx\n",
    "    # Add special tokens if they are not in the vocabulary\n",
    "    if \"<sos>\" not in vocab:\n",
    "        vocab[\"<sos>\"] = len(vocab)\n",
    "    if \"<eos>\" not in vocab:\n",
    "        vocab[\"<eos>\"] = len(vocab)\n",
    "    if \"<unk>\" not in vocab:\n",
    "        vocab[\"<unk>\"] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Load vocabulary and map tokens to integer IDs\n",
    "vocab = load_vocab(VOCAB_FILE)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Use the <sos> token ID for the decoder input in training\n",
    "SOS_TOKEN_ID = vocab[\"<sos>\"]\n",
    "\n",
    "# Step 2: Load and process BPE tokenized data\n",
    "def load_bpe_data(file_path, vocab):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Convert each token to its ID, or to <unk> if it's not in the vocabulary\n",
    "            token_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in line.strip().split()]\n",
    "            data.append(token_ids)\n",
    "    # Pad sequences to have the same length\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(data, padding=\"post\")\n",
    "\n",
    "# Load English and German sequences\n",
    "input_sequences = load_bpe_data(TEST_EN_TOY, vocab)\n",
    "target_sequences = load_bpe_data(TEST_DE_TOY, vocab)\n",
    "\n",
    "# Step 3: Create a TensorFlow dataset with batching\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_sequences, target_sequences))\n",
    "dataset = dataset.shuffle(buffer_size=len(input_sequences)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(f\"Loaded dataset with {len(input_sequences)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = BASE_DIR + \"checkpoints\"\n",
    "CHECKPOINT_FILEPATH = CHECKPOINT_DIR + \"/seq2seq_weights\"\n",
    "\n",
    "import os\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x)\n",
    "        return output, state\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, enc_output, hidden):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n",
    "\n",
    "# Instantiate encoder, decoder, and optimizer\n",
    "encoder = Encoder(vocab_size, EMBEDDING_DIM, HIDDEN_UNITS)\n",
    "decoder = Decoder(vocab_size, EMBEDDING_DIM, HIDDEN_UNITS)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "\n",
    "# Define the loss function\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# Step 5: Modify Training Step Function to correct the shape mismatch\n",
    "@tf.function\n",
    "def train_step(input_seq, target_seq):\n",
    "    loss = 0\n",
    "    print(\"Running train step...\") \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(input_seq)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([SOS_TOKEN_ID] * BATCH_SIZE, 1)  # Start token\n",
    "\n",
    "        for t in range(1, target_seq.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, enc_output, dec_hidden)\n",
    "            \n",
    "            # Remove the extra dimension from predictions to match target shape\n",
    "            predictions = tf.squeeze(predictions, axis=1)  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss += loss_function(target_seq[:, t], predictions)\n",
    "            \n",
    "            # Use the true target as the next input to the decoder\n",
    "            dec_input = tf.expand_dims(target_seq[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(target_seq.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10  # Set the number of epochs as needed\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (input_seq, target_seq)) in enumerate(dataset):\n",
    "        batch_loss = train_step(input_seq, target_seq)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss.numpy() / (batch + 1)}\")\n",
    "    \n",
    "    # Save weights at the end of each epoch\n",
    "    checkpoint.save(file_prefix=CHECKPOINT_FILEPATH)\n",
    "    print(f\"Model weights saved for epoch {epoch + 1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
