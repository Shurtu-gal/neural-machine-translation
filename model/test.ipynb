{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "CHECKPOINT_DIR = \"./checkpoints\"  # Replace with your checkpoint directory path\n",
    "TRAIN_EN_FILE = \"data/wmt14_de_en/train.tok.clean.bpe.32000.en\"  # Update with correct path\n",
    "TRAIN_DE_FILE = \"data/wmt14_de_en/train.tok.clean.bpe.32000.de\"  # Update with correct path\n",
    "BPE_CODES_FILE = \"data/wmt14_de_en/bpe.32000\"  # Update with correct path\n",
    "VOCAB_FILE_EN = \"data/wmt14_de_en/vocab.bpe.32000\"  # Update with correct path\n",
    "VOCAB_FILE_DE = \"data/wmt14_de_en/vocab.bpe.32000\"  # Update with correct path\n",
    "\n",
    "# Vocabulary and BPE model\n",
    "with open(VOCAB_FILE_EN, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab_en = {line.strip(): idx for idx, line in enumerate(f)}\n",
    "with open(VOCAB_FILE_DE, \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab_de = {line.strip(): idx for idx, line in enumerate(f)}\n",
    "\n",
    "# Reverse vocabulary for German (to decode token IDs back to words)\n",
    "inv_vocab_de = {idx: word for word, idx in vocab_de.items()}\n",
    "\n",
    "# Initialize BPE encoding\n",
    "bpe = BPE(open(BPE_CODES_FILE, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Special tokens\n",
    "SOS_TOKEN_ID = vocab_en.get(\"<sos>\", 1)  # Use 1 if <sos> isn't in vocab\n",
    "EOS_TOKEN_ID = vocab_en.get(\"<eos>\", 2)  # Use 2 if <eos> isn't in vocab\n",
    "UNK_TOKEN_ID = vocab_en.get(\"<unk>\", 3)  # Use 3 if <unk> isn't in vocab\n",
    "\n",
    "# Define encoder and decoder models (assuming you've implemented them previously)\n",
    "encoder = Encoder(vocab_size=len(vocab_en), embedding_dim=256, enc_units=512, batch_sz=1)\n",
    "decoder = Decoder(vocab_size=len(vocab_de), embedding_dim=256, dec_units=512, batch_sz=1)\n",
    "\n",
    "# Checkpoint to restore the model weights\n",
    "checkpoint = tf.train.Checkpoint(encoder=encoder, decoder=decoder)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(CHECKPOINT_DIR)).expect_partial()\n",
    "print(\"Model weights restored from checkpoint.\")\n",
    "\n",
    "### Utility Functions\n",
    "\n",
    "def preprocess_sentence(sentence, vocab, bpe):\n",
    "    \"\"\"\n",
    "    Tokenize and encode the sentence with BPE, converting to vocab IDs.\n",
    "    \"\"\"\n",
    "    sentence_bpe = bpe.process_line(sentence)  # Apply BPE\n",
    "    tokens = sentence_bpe.split()  # Tokenize\n",
    "    token_ids = [vocab.get(token, UNK_TOKEN_ID) for token in tokens]\n",
    "    return tf.convert_to_tensor([token_ids], dtype=tf.int32)  # Convert to tensor\n",
    "\n",
    "def decode_tokens(tokens, inv_vocab):\n",
    "    \"\"\"\n",
    "    Decode a sequence of token IDs back to words.\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for token in tokens:\n",
    "        word = inv_vocab.get(token, \"<unk>\")\n",
    "        if word == \"<eos>\":\n",
    "            break\n",
    "        words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def translate_sentence(sentence, encoder, decoder):\n",
    "    \"\"\"\n",
    "    Translate an English sentence to German.\n",
    "    \"\"\"\n",
    "    # Preprocess and encode the English sentence\n",
    "    input_seq = preprocess_sentence(sentence, vocab_en, bpe)\n",
    "    enc_output, enc_hidden = encoder(input_seq)\n",
    "    \n",
    "    # Initialize the decoder with the <sos> token\n",
    "    dec_input = tf.expand_dims([SOS_TOKEN_ID], 0)\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    # Store the translation\n",
    "    result = []\n",
    "    \n",
    "    # Generate translation token-by-token\n",
    "    for t in range(50):  # Set a max length for the output sentence\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, enc_output, dec_hidden)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        # Stop if the <eos> token is predicted\n",
    "        if predicted_id == EOS_TOKEN_ID:\n",
    "            break\n",
    "        \n",
    "        # Append the predicted token to the result\n",
    "        result.append(predicted_id)\n",
    "        \n",
    "        # Use the predicted token as the next input to the decoder\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    # Decode the sequence of token IDs into a German sentence\n",
    "    translated_sentence = decode_tokens(result, inv_vocab_de)\n",
    "    return translated_sentence\n",
    "\n",
    "### Translate a Sample Sentence\n",
    "\n",
    "sample_sentence = \"Hello, how are you?\"\n",
    "translated_text = translate_sentence(sample_sentence, encoder, decoder)\n",
    "\n",
    "print(\"English:\", sample_sentence)\n",
    "print(\"German:\", translated_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
